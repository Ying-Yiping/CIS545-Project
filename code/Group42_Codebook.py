# -*- coding: utf-8 -*-
"""0426_Final_Draft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16UPR9mBkzO7VEqgBsncI_b4MMGH6MOap
"""

!pip install geopandas
!pip install contextily
!pip install hvplot
!pip install esri2gpd
!pip install carto2gpd
!pip install cenpy
!pip install osmnx
!pip install panel

!pip install panel
import panel as pn
pn.extension('vega')

# Ignore Warning
import warnings
warnings.filterwarnings("ignore")

# Data Wrangling
import numpy as np
import pandas as pd
import geopandas as gpd
import time
import datetime as dt
import requests
from statistics import mean

# Plot
from matplotlib import pyplot as plt
import contextily as ctx
import hvplot.pandas
import esri2gpd
import carto2gpd
import seaborn as sns
import altair as alt
import holoviews as hv

# ACS
import cenpy

# OpenStreetMap
import osmnx as ox

# Neighbors
from sklearn.neighbors import NearestNeighbors

# Panel
import panel as pn
pn.extension('vega')

# Models
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

# Model selection
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV

# Pipelines
from sklearn.pipeline import make_pipeline

# Preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder

# Neighbors
from sklearn.neighbors import NearestNeighbors

# ARIMA
import statsmodels.api as sm
from statsmodels.tsa.stattools import pacf
from dateutil.relativedelta import relativedelta
import datetime
from statsmodels.tsa.arima_model import ARMA
import pmdarima as pm 
from statsmodels.tsa.stattools import acf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from numpy import log 
from statsmodels.tsa.stattools import adfuller, kpss
from statsmodels.tsa.arima_model import ARIMA
from dateutil.parser import parse
import matplotlib.cm as cm
from statsmodels.tsa.statespace.sarimax import SARIMAX

# Pytorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.functional import normalize

# packages needed for the GRU part:
!pip install hvplot
!pip install geopandas
!pip install altair
!pip install panel
!pip install esri2gpd

import requests
import warnings
warnings.filterwarnings("ignore")

import panel as pn
pn.extension('vega')

import pandas as pd
import geopandas as gpd
import numpy as np
import altair as alt
import matplotlib.pyplot as plt
import hvplot.pandas
import esri2gpd

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.functional import normalize

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

"""# CIS 545 Final Project: _**Indego Shared Bike Demand Forecast**_

_Spring, 2023_

Yiping Ying, Ying Xu, Troy Yang

## **I. Motivation**

Bike sharing is a new type of micro-transportation that has emerged in urban areas in recent years. Its appearance provides a new way to solve the “last mile” problem. However, while bringing us convenience, shared bicycles also have some difficult problems to solve, among which the re-balancing problem is more prominent.

The problem of re-balancing arises because the number of shared bicycles is not evenly distributed among various sites. In some remote sites, people ride bikes out of the site, but no one returns the shared bicycles. A common solution is that the shared bicycle companies will send some trucks to transport the shared bicycles to these sites to complete the supplement of shared bicycles. The question with this type of solution is how to distribute the volume of these trucks to maximize efficiency?

In general, there are two steps in truck-based rebalancing approaches, i.e., demand prediction and station rebalancing. First, it is crucial to accurately predict the demand at each station to foresee the bike and dock availability in the future. Second, it is important to design effective strategies for truck operators to reposition bikes among stations.

This project will focus on the first step of the problem by taking Indego, a bike-sharing system in the Philadelphia area, as a research sample, on which we will use different models (OLS, ARIMA, Neural Networks, etc.) to make accurate demand forecasts. Indego started operation on April 23, 2015, with 125 stations and 1,000 bicycles. After 7 years of development, the function of this system has tended to be comprehensive, which is appropriate for research. In this analysis, we select a 5-week period from May 17 to June 20, 2021 for temporal/spatial analysis.

## **II. Exploratory Data Analysis**

###  2.1 Getting trip data for the Indego bike share

- Available by quarter from: https://www.rideindego.com/about/data/

- Pulled historic trip data from 2021 quarter 2 (available in the data folder)

The data page also includes the live status of all of the stations in the system.

API endpoint: http://www.rideindego.com/stations/json

**Two important columns**:

- `totalDocks`: the total number of docks at each station;

- `kioskId`: the unique identifier for each station.
"""

# API endpoint
API_endpoint = "http://www.rideindego.com/stations/json"

# Get the JSON
stations_geojson = requests.get(API_endpoint).json()

# Convert to a GeoDataFrame
stations = gpd.GeoDataFrame.from_features(stations_geojson, crs="EPSG:4326")

stations.head(n=3)

"""- ##### ***Plot1***: Stations, colored by the number of docks"""

fig, ax = plt.subplots(figsize=(12, 8))

# stations
stations_3857 = stations.to_crs(epsg=3857)
stations_3857.plot(ax=ax, column='totalDocks', legend=True, cmap="GnBu")

# plot the basemap underneath
ctx.add_basemap(ax=ax, crs=stations_3857.crs, source=ctx.providers.CartoDB.Positron)
#ctx.add_basemap(ax=ax, source=ctx.providers.Stamen.TonerLabels)

ax.set_title('Number of Docks in Each Station, Philadelphia 2021')
ax.set_axis_off()

"""- ##### ***Plot2***: Number of Docks in Each Bike Station

Get the geometry polygon for the Philadelphia city limits:
"""

# Download the Philadelphia city limits
url = "https://services.arcgis.com/fLeGjb7u4uXqeF9q/arcgis/rest/services/City_Limits/FeatureServer/0"
city_limits = esri2gpd.get(url).to_crs(epsg=3857)

# Get the geometry from the city limits
city_limits_outline = city_limits.to_crs(epsg=4326).squeeze().geometry

city_limits_outline

background = alt.Chart(city_limits).mark_geoshape(
    fill='lightgray',
    stroke='white'
).project(
    type='identity', 
      reflectY=True).properties(
    width=360,
    height=360)

station_alt = alt.Chart(stations_3857).\
      mark_geoshape().\
      encode(
      tooltip=['name:N','id:Q', 'totalDocks:Q'],
      color=alt.Color('totalDocks:Q', scale=alt.Scale(scheme="greenblue"),title='Count')).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='Number of Docks in Each Bike Station, Philadelphia 2021')

background + station_alt

station_docks = background + station_alt

"""**Load historic trips data**"""

all_trips = pd.read_csv("C:/Users/yyp/Desktop/Upenn/MUSA/CPLN6800/data/indego-trips-2021-q2.csv")

# Change time type:
for col in ['start_time', 'end_time']:
    all_trips[col] = all_trips[col].astype("datetime64[ns]")

"""**Select:**

- Trip records between May 15, 2021 and June 20, 2021 (Week 20 - 24)

- Trip duration less than 3 hours.
"""

# Trip records between May 15, 2021 and June 20, 2021 (Week 20 - 24)
all_trips['week'] = all_trips['start_time'].dt.strftime("%W")
all_trips = all_trips[(all_trips['week']>='20')&(all_trips['week']<='24')]

# Trip duration less than 3 hours
all_trips = all_trips[all_trips['duration']<=180]

len(all_trips)

"""- ##### ***Plot3***: Number of Rides for each time of the day"""

record = all_trips

# Get the weekday and the start hour
record['weekday'] = record['start_time'].dt.strftime("%A")
record['hour'] = record['start_time'].dt.strftime("%H")

# Summary
plot1 = record.groupby(['weekday','hour'],as_index=False)['trip_id'].count().rename(columns={'trip_id':'trip_count'})

# Cast the labels
cats = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
plot1['weekday'] = pd.Categorical(plot1['weekday'], categories=cats, ordered=True)
plot1 = plot1.sort_values('weekday')

cats2 = ['00','01', '02', '03', '04', '05', '06', '07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']
plot1['hour'] = pd.Categorical(plot1['hour'], categories=cats2, ordered=True)
plot1 = plot1.sort_values('hour')

# Plot
sns.set_theme(style="darkgrid")
fig, ax = plt.subplots(figsize=(18, 5))

sns.lineplot(x="hour", y="trip_count",hue="weekday",style="weekday",markers=True,data=plot1).set(title='Number of Rides for each Time of Day')
#plt.xticks(rotation=45)
ax.set(xlabel='hour', ylabel='Number of Rides')

"""- ##### ***Plot4***: Trip Duration Density"""

sns.set_theme(style="darkgrid")

fig, ax = plt.subplots(figsize =(7, 3))
sns.distplot(all_trips['duration'], bins= 20 ,ax=ax)
ax.set_title('Indego Shared Bike Trip Duration Density, May/June, 2021',fontsize=10,color='black')
ax.set_xlabel('Trip Duration (unit: minute)', fontsize = 10)
plt.show()

"""- ##### ***Plot5***: Number of Daily trips"""

# Get the start hour
freq = '1h'

record['interval60'] = record['start_time'].dt.floor(freq)

# Get the week
record['tag'] = record['week'].apply(lambda x: 'test' if ((x=='23')|(x=='24')) else 'train')

# Summary
record2 = record.groupby(['tag','interval60'],as_index=False)['trip_id'].count().rename(columns={'trip_id':'count'})

# Memorial Day of 2021: 5/31
fest = dt.datetime(2021,5,31,0,0,0)

plt.figure(figsize=(12,3))
#palette = sns.color_palette("muted", 2)
trips_by_date_lineplot = sns.lineplot(x='interval60',y = 'count',hue = 'tag',data= record2,palette=sns.color_palette("Blues", n_colors=2))

trips_by_date_lineplot.axvline(fest, linestyle='--',color = 'grey', label = 'Memorial Day')
plt.title('Number of Indego Shared Bike Trips by Date, May/June 2021', fontsize = 12)
plt.xlabel('Date', fontsize = 10)
plt.ylabel('Number of trips', fontsize = 10)
plt.xticks(rotation = 30,fontsize=8)
plt.yticks(fontsize=8)

plt.legend()
plt.show()

"""- ##### ***Plot6***: Average Number of Rides per Hour in Weekday/Weekend"""

# Weekday/Weekend
record['tag2'] = record['weekday'].apply(lambda x:'Weekend' if((x=='Sunday')|(x=='Saturday')) else 'Weekday')

# Summary
record3 = record.\
        groupby(['tag2','hour'],as_index=False)['trip_id'].\
        count().\
        rename(columns={'trip_id':'Count'})

# Average number of trips 
record3['Count'] = record3['Count'].apply(lambda x:x/5 if(x=='Weekday') else x/2)

# Reset the order of labels
cats2 = ['00','01', '02', '03', '04', '05', '06', '07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']
record3['hour'] = pd.Categorical(record3['hour'], categories=cats2, ordered=True)
record3= record3.sort_values('hour')

# Plot
sns.set_theme(style="darkgrid")
fig, ax = plt.subplots(figsize=(18, 4))

sns.lineplot(x="hour", y="Count",hue="tag2",style="tag2",markers=True,data=record3).set(title='Average Number of Rides per Hour in Weekday/Weekend, May/June 2021')
#plt.xticks(rotation=45)
ax.set(xlabel='hour', ylabel='Number of Rides')

"""- ##### ***Charts***: Total trips by start/end stations

**total trips by starting station**
"""

start_trips = (
    all_trips.groupby("start_station").size().reset_index(name="total_start_trips")
)

start_trips.sort_values(['total_start_trips'],ascending=False).head()

# Now merge in the geometry for each station
bike_data = (
    stations[["name","geometry", "kioskId"]]
    .merge(start_trips, left_on="kioskId", right_on="start_station")
    .to_crs(epsg=3857)
)

"""**total trips by starting station**"""

end_trips = (
    all_trips.groupby("end_station").size().reset_index(name="total_end_trips")
)

end_trips.sort_values(['total_end_trips'],ascending=False).head()

# Now merge in the geometry for each station
bike_data = (
    bike_data
    .merge(end_trips, left_on="kioskId", right_on="end_station")
    .to_crs(epsg=3857)
)

bike_data.head()

"""**Plot the stations, colored by the number of start trips**"""

background = alt.Chart(city_limits).mark_geoshape(
    fill='lightgray',
    stroke='lightgray'
).project(
    type='identity', 
      reflectY=True).properties(
    width=360,
    height=360)

station_alt = alt.Chart(bike_data).\
      mark_geoshape().\
      encode(
      tooltip=['name:N','total_start_trips:Q'],
      color=alt.Color('total_start_trips:Q', scale=alt.Scale(scheme="greenblue"),title='Count')).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='Number of Start Trips in Each Bike Station, Philadelphia 2021')

background + station_alt

background = alt.Chart(city_limits).mark_geoshape(
    fill='lightgray',
    stroke='lightgray'
).project(
    type='identity', 
      reflectY=True).properties(
    width=360,
    height=360)

end_alt = alt.Chart(bike_data).\
      mark_geoshape().\
      encode(
      tooltip=['name:N','total_end_trips:Q'],
      color=alt.Color('total_end_trips:Q', scale=alt.Scale(scheme="greenblue"),title='Count')).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='Number of End Trips in Each Bike Station, Philadelphia 2021')

background + end_alt

# Tab Generating
Start_trips = background + station_alt
End_trips = background + end_alt

# Combination
docks_count = pn.panel(station_docks)
trips_count = pn.panel(Start_trips)
end_count = pn.panel(End_trips)

tabs0 = pn.Tabs(
    ('docks', docks_count),
    ('start trips', trips_count),
    ('end trips', end_count)
)

# This cannot be displayed on jupyter notebook, when downloaded, it can be shown on the browser.
tabs0

#tabs0.save('tabs0.html')

"""### 2.2 Feature Engineering

- **Internal**

  - e.g., the number of docks per station;

- **Demographic (Census)**

  - e.g., population, income, percent commuting by car, percent with bachelor's degree or higher, etc.

- **Amenities/Disamenities**

  - e.g., distance to nearest restaurants, parks, within Center City, etc

- **Transportation network**

  - e.g., distance to nearest bus stop, interesection nodes, nearest subway stop

- **Neighboring stations**

  - e.g., average trips of nearest stations, distance to nearest stations

----------------------------------------------

#### 2.2.1 Internal Characteristics
"""

bike_data = bike_data.merge(stations[["kioskId", "totalDocks"]], on="kioskId")
bike_data.head()

len(bike_data)

"""#### 2.2.2 Census demographic data

- ##### ***Import***: from API
"""

# ACS-5 
acs = cenpy.remote.APIConnection("ACSDT5Y2021")

variables = ["NAME", 
        # Total transportation
        "B08134_001E", 
        # car
        "B08134_011E",
        #total, male, female
        'B01003_001E','B08014_008E','B08014_015E',
        #2 people family household, nonfamily household, total household
        'B11016_003E', 'B11016_009E','B11016_001E',
        # student
        'B99142_002E',
        # educated
        'B99151_002E',
        # employed
        'B23025_004E',
        #MEANS OF TRANSPORTATION TO WORK BY AGE
        'B08101_041E','B08101_049E','B08101_017E','B08101_009E',
        'B08101_033E','B08101_025E','B06011_001E','B08103_001E']


# Pull data by census tract
census_data = acs.query(
    cols=variables,
    geo_unit="tract:*",
    geo_filter={"state": "42","county": "101"},
)

# Convert to the data to floats
for col in variables[1:]:
    census_data[col] = census_data[col].astype(float)

# The percent commuting by car
census_data["percent_car"] = census_data["B08134_011E"] / census_data["B08134_001E"]

# Ratios:
census_data = census_data.\
        eval('gender_ratio = B08014_008E / (B08014_015E + 1e-5)').\
        eval('smallNonFamily = (B11016_003E + B11016_009E)/(B11016_001E + 1e-5)').\
        eval('student_ratio = B99142_002E / (B01003_001E + 1e-5)').\
        eval('educated_ratio = B99151_002E / (B01003_001E + 1e-5)').\
        eval('employed_ratio = B23025_004E / (B01003_001E + 1e-5)')

census_data = census_data.\
        rename(columns={'B08101_041E':'other',
                'B08101_049E':'Worked from home', 
                'B08101_017E':'carpooled', 
                'B08101_009E':'drove alone', 
                'B08101_033E':'walk', 
                'B08101_025E': 'public transit',
                'B06011_001E':'income',
                'B08103_001E':'median age',
                'B01003_001E':'totalpop',}).\
        drop(columns=['B08134_011E',
               'B08134_001E',
               'B99142_002E',
               'B99151_002E',
               'B23025_004E',
               'B08014_008E',
               'B08014_015E',
               'B11016_003E',
               'B11016_009E',
               'B11016_001E'])

"""- ##### Merge with census tract geometries for Philadelphia"""

# Select the unit
acs.set_mapservice("tigerWMS_Census2020")

block_group_layer = acs.mapservice.layers[6]

# Use SQL to return geometries only for Philadelphia County in PA
where_clause = "STATE = '42' AND COUNTY = '101'"

# Query for tracts
block_groups = esri2gpd.get(block_group_layer._baseurl, where=where_clause)

# Merge
census_data = block_groups.merge(
    census_data,
    left_on=["STATE", "COUNTY", "TRACT"],
    right_on=["state", "county", "tract"],
)

"""- ##### ***Charts***: Interactive

I. Median Income vs Transit Mode Selection

"""

# Median Income:
median_income = round(census_data[census_data['income']>0]['income'].mean(),2)
census_data['income'] = census_data['income'].apply(lambda x:median_income if(x<0) else x)

# Median Age:
median_age = round(census_data[census_data['median age']>0]['median age'].mean(),2)
census_data['median age'] = census_data['median age'].apply(lambda x:median_age if(x<0) else x)

# Select columns
philly_mode = census_data[census_data.columns[30:37]]
philly_mode= philly_mode.melt(id_vars= "income", var_name='mode' )
philly_mode = philly_mode.rename(columns={'value': "num/tract"})
philly_mode = philly_mode.rename(columns={'value': "num/tract"})

# Select interval
brush = alt.selection_interval()

# Interactive Chart:
points = alt.Chart(philly_mode).\
     mark_point().\
     encode(
     x='income:Q',
     y='num/tract:Q',
     color=alt.condition(brush, 'mode:N', alt.value('lightgray'))
        ).\
     add_selection(brush).\
     properties(
     title='Median Income vs Transit Mode Selection in Philadelphia'
     )

bars = alt.Chart(philly_mode).\
    mark_bar().\
    encode(
    y='mode:N',
    color='mode:N',
    x='average(num/tract):Q').\
    transform_filter(brush)

alt1 = points & bars

alt1

#alt1.save("Philly_mode_income.json")

"""II. Median Age vs Transit Mode Selection"""

# Select Columns:
age_mode = census_data[census_data.columns[30:38]]
age_mode = age_mode.drop(columns=['income'])
age_mode= age_mode.melt(id_vars= "median age", var_name='mode' )
age_mode = age_mode.rename(columns={'value': "num/tract"})

# Select Interval:
brush = alt.selection_interval()

# Interactive Chart:
points = alt.Chart(age_mode).\
     mark_point().\
     encode(
     x='median age:Q',
     y='num/tract:Q',
     color=alt.condition(brush, 'mode:N', alt.value('lightgray'))).\
     add_selection(brush).\
     properties(
    title='Median Age vs Transit Mode Selection in Philadelphia')

bars = alt.Chart(age_mode).\
    mark_bar().\
    encode(
    y='mode:N',
    color='mode:N',
    x='sum(num/tract):Q').\
    transform_filter(brush)

alt2 = points & bars

alt2

#alt2.save("Philly_mode_age.json")

"""- ##### ***Chroplethes***"""

city = census_data.to_crs("epsg:3857")

choropleth_age = alt.Chart(city).\
      mark_geoshape().\
      encode(
      tooltip=['income:Q', 'median age:Q','walk:Q','public transit:Q','drove alone:Q','carpooled:Q','Worked from home:Q'],
      color=alt.Color('median age:Q', scale=alt.Scale(scheme="greenblue"))).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='Median age in Philadelphia, 2021')

choropleth_age

choropleth_income = alt.Chart(city).\
      mark_geoshape().\
      encode(
      tooltip=['income:Q', 'median age:Q','walk:Q','public transit:Q','drove alone:Q','carpooled:Q','Worked from home:Q'],
      color=alt.Color('income:Q', scale=alt.Scale(scheme="greenblue"))).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='Median Income in Philadelphia, 2021')
  
choropleth_income

choropleth_population = alt.Chart(city).\
      mark_geoshape().\
      encode(
      tooltip=['totalpop:Q','income:Q', 'median age:Q','walk:Q','public transit:Q','drove alone:Q','carpooled:Q','Worked from home:Q'],
      color=alt.Color('totalpop:Q', scale=alt.Scale(scheme="greenblue"),title='Count')).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='Population in Philadelphia, 2021')
  
choropleth_population

# Imput the features with 'percent'
city['percent_car'] = city['percent_car'].apply(lambda x:x*100)
city['student_ratio'] = city['student_ratio'].apply(lambda x:x*100)
city['educated_ratio'] = city['educated_ratio'].apply(lambda x:x*100)
city['employed_ratio'] = city['employed_ratio'].apply(lambda x:x*100)

choropleth_pct_car = alt.Chart(city).\
      mark_geoshape().\
      encode(
      tooltip=['percent_car:Q','income:Q', 'median age:Q','walk:Q','public transit:Q','drove alone:Q','carpooled:Q','Worked from home:Q'],
      color=alt.Color('percent_car:Q', scale=alt.Scale(scheme="greenblue"),title='Car%')).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='Percent Car in Philadelphia, 2021')
  
choropleth_pct_car

choropleth_student = alt.Chart(city).\
      mark_geoshape().\
      encode(
      tooltip=['student_ratio:Q','income:Q', 'median age:Q','walk:Q','public transit:Q','drove alone:Q','carpooled:Q','Worked from home:Q'],
      color=alt.Color('student_ratio:Q', scale=alt.Scale(scheme="greenblue"),title='Student%')).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='Student Ratio in Philadelphia, 2021')
  
choropleth_student

choropleth_educated = alt.Chart(city).\
      mark_geoshape().\
      encode(
      tooltip=['educated_ratio:Q','income:Q', 'median age:Q','walk:Q','public transit:Q','drove alone:Q','carpooled:Q','Worked from home:Q'],
      color=alt.Color('educated_ratio:Q', scale=alt.Scale(scheme="greenblue"),title='Educated%')).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='Eduacted Ratio in Philadelphia, 2021')
  
choropleth_educated

choropleth_employed = alt.Chart(city).\
      mark_geoshape().\
      encode(
      tooltip=['employed_ratio:Q','income:Q', 'median age:Q','walk:Q','public transit:Q','drove alone:Q','carpooled:Q','Worked from home:Q'],
      color=alt.Color('employed_ratio:Q', scale=alt.Scale(scheme="greenblue"),title='Employed%')).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='Employed Ratio in Philadelphia, 2021')
  
choropleth_employed

"""- Generate Panel"""

# Combination
choro_1 = pn.panel(choropleth_age)
choro_2 = pn.panel(choropleth_income)
choro_3 = pn.panel(choropleth_population)
choro_4 = pn.panel(choropleth_pct_car)
choro_5 = pn.panel(choropleth_student)
choro_6 = pn.panel(choropleth_educated)
choro_7 = pn.panel(choropleth_employed)

tabs = pn.Tabs(
    ('Age', choro_1),
    ('Income', choro_2),
    ('Population', choro_3),
    ('Car%', choro_4),
    ('Student%', choro_5),
    ('Educated%', choro_6),
    ('Employed%', choro_7)
)
tabs

#tabs.save('tabs.html')

"""- ##### Merge Together

Merge the census data into our dataframe of features by spatially joining the stations and the block groups.
"""

bike_data = gpd.sjoin(
    bike_data,
    census_data.to_crs(bike_data.crs)[['geometry',
                    'percent_car',
                    'totalpop',
                    'student_ratio',
                    'educated_ratio',
                    'employed_ratio'
                    ]],
    predicate="within",
).drop(labels=["index_right"], axis=1)

"""#### 2.2.3 Amenities/Disamenities

Add two new features:

1.Distances to the nearest 10 restaurants from Open Street Map;

2.Whether the station is located within Center City.
"""

city_limits_outline

"""- ##### ***Feature1***: Restaurants

Search https://wiki.openstreetmap.org/wiki/Map_Features for OSM identifier of restaurants.
"""

restaurants = ox.geometries_from_polygon(
    city_limits_outline, tags={"amenity": "restaurant"}
).to_crs(epsg=3857)

restaurants.head()

"""- ##### ***Function***: Get x/y values for the stations"""

def get_xy_from_geometry(df):
    """
    Return a numpy array with two columns, where the 
    first holds the `x` geometry coordinate and the second 
    column holds the `y` geometry coordinate
    
    Note: this works with both Point() and Polygon() objects.
    """
    # NEW: use the centroid.x and centroid.y to support Polygon() and Point() geometries 
    x = df.geometry.centroid.x
    y = df.geometry.centroid.y
    
    return np.column_stack((x, y)) # stack as columns

stationsXY = get_xy_from_geometry(bike_data)

# STEP 1: x/y coordinates of restaurants (in EPGS=3857)
restsXY = get_xy_from_geometry(restaurants.to_crs(epsg=3857))

# STEP 2: Initialize the algorithm
nbrs = NearestNeighbors(n_neighbors=10)

# STEP 3: Fit the algorithm on the "neighbors" dataset
nbrs.fit(restsXY)

# STEP 4: Get distances for stations to neighbors
restsDists, restsIndices = nbrs.kneighbors(stationsXY)

# STEP 5: add back to the original dataset
bike_data['logDistRests'] = np.log10(restsDists.mean(axis=1))

bike_data.head()

"""- ##### ***Plot***: Mean Distance to Nearest 10 Restaurants (log transformed)"""

fig, ax = plt.subplots(figsize=(6,6))

# stations
bike_data.plot(ax=ax, column='logDistRests', legend=True)

# plot the basemap underneath
ctx.add_basemap(ax=ax, crs=bike_data.crs, source=ctx.providers.CartoDB.Positron)

#ax.set_title('Mean Distance to Nearest 10 Restaurants (log transformed)')
ax.set_axis_off()

"""- ##### ***Feature2***: Within the Center City business district

Available from [OpenDataPhilly](https://opendataphilly.org/).
"""

url = "http://data.phl.opendata.arcgis.com/datasets/95366b115d93443eae4cc6f498cb3ca3_0.geojson"
ccbd = gpd.read_file(url).to_crs(epsg=3857)

ccbd_geo = ccbd.squeeze().geometry

ccbd_geo

# Add the new feature: 1 if within CC and 0 otherwise
bike_data['within_centerCity'] = bike_data.geometry.within(ccbd_geo).astype(int)

"""#### 2.2.4 Transportation Network

Add a feature that calculates the distance to the nearest intersections.

- ##### ***Feature***: Average distance to the 3 nearest intersections
"""

# Boundary of the stations
xmin, ymin, xmax, ymax = bike_data.to_crs(epsg=4326).total_bounds

G = ox.graph_from_bbox(ymax, ymin, xmax, xmin, network_type='bike')

#ox.plot_graph(G, figsize=(8, 8), node_size=0);

""" Extract the nodes (intersections) from the graph into a GeoDataFrame:"""

intersections = ox.graph_to_gdfs(G, nodes=True, edges=False).to_crs(epsg=3857)

intersections.head()

"""compute the average distance to the 3 nearest intersections:"""

# STEP 1: x/y coordinates of restaurants (in EPGS=3857)
intersectionsXY = get_xy_from_geometry(intersections.to_crs(epsg=3857))

# STEP 2: Initialize the algorithm
nbrs = NearestNeighbors(n_neighbors=3)

# STEP 3: Fit the algorithm on the "neighbors" dataset
nbrs.fit(intersectionsXY)

# STEP 4: Get distances for stations to neighbors
interDists, interIndices = nbrs.kneighbors(stationsXY)

# STEP 5: add back to the original dataset
bike_data['logIntersectionDists'] = np.log10(interDists.mean(axis=1))

"""- ##### ***Plot***: Stations, coloring by the new feature (distance to intersections)"""

fig, ax = plt.subplots(figsize=(6,6))

# stations
bike_data.plot(ax=ax, column='logIntersectionDists', legend=True)

# plot the basemap underneath
ctx.add_basemap(ax=ax, crs=bike_data.crs, source=ctx.providers.CartoDB.Positron)

ax.set_axis_off()

"""#### 2.2.5 Neighboring Stations

- ##### ***Feature: Spatial Lag***

a feature that encodes the fact that demand for a specific station is likely related to the demand in neighboring stations.

We will add two new features:

1. The average distance to the nearest 5 stations

2. The average trip total for the nearest 5 stations

First, find the nearest 5 stations:
"""

N = 5
k = N + 1
nbrs = NearestNeighbors(n_neighbors=k)
nbrs.fit(stationsXY)

stationDists, stationIndices = nbrs.kneighbors(stationsXY)

"""**Note:**

- We are matching the stations to themselves to find the nearest neighbors

- The closest match will always be the same station (distance of 0)

- So we fit for _k_ + 1 neighbors and will remove the closest neighbor

The log of the distances to the 5 nearest stations:
"""

bike_data["logStationDists"] = np.log10(stationDists[:, 1:].mean(axis=1))

"""- ##### ***Plot1***: Distances to the 5 nearest stations (log transformed)"""

fig, ax = plt.subplots(figsize=(6,6))

# stations
bike_data.plot(ax=ax, column='logStationDists', legend=True)

# plot the basemap underneath
ctx.add_basemap(ax=ax, crs=bike_data.crs, source=ctx.providers.CartoDB.Positron)

ax.set_axis_off()

total_start_trips = bike_data['total_start_trips'].values

# get the trips for the 5 nearest neighbors (ignoring first match)
neighboring_trips = total_start_trips[stationIndices[:, 1:]]

# Add to features
bike_data['laggedTrips'] = neighboring_trips.mean(axis=1)

"""- ##### ***Plot2***: Check the correlation between the trip counts and the spatial lag"""

sns.regplot(x=bike_data['laggedTrips'], y=bike_data['total_start_trips']);

"""#### 2.2.6 Correlation Matrix

**Correlations of all of our features**
"""

sns.set_theme(style="white")

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Select Features
feature_cols = [
    "total_start_trips",
    "totalDocks",
    "totalpop",
    "student_ratio",
    "educated_ratio",
    "employed_ratio",
    "percent_car",
    "logDistRests",
    "within_centerCity",
    "logIntersectionDists",
    "logStationDists",
    "laggedTrips",
]

# Compute the correlation matrix
corr= bike_data[feature_cols].corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True,annot=True, linewidths=.5, cbar_kws={"shrink": .5})

"""### 2.3 Panel Generating

In this section, we generate a time panel with an interval of 1 hour for further modeling.

- #### 2.3.1 ***Time Panel***: 1 hour
"""

# 1 hour
onehour = dt.timedelta(hours=1)
# 15 min
#onequarter = dt.timedelta(minutes=15)

# start date
startdate = dt.datetime(2021,5,17,0,0,0)

## Panel Generation
times=[]

for i in range(24*35):
    t= startdate+onehour*i
    t=pd.to_datetime(t)
    times.append(t)
    
times_df = pd.DataFrame({"interval60":times})

# Select start stations
stations = bike_data[['start_station']]

id_df = pd.DataFrame({"station_id":stations['start_station'].unique()})

# Join together
times_df['key'] = 0
id_df['key'] = 0
panel_60 = id_df.merge(times_df, on='key', how='outer').drop("key",axis=1)

# Set floor time in the `all_trips`
freq = '1h'

all_trips['interval60'] = all_trips['start_time'].dt.floor(freq)

# count the number of trips in each interval, every station
trip60 = all_trips.groupby(['start_station','interval60'],as_index=False)['trip_id'].count().rename(columns={'start_station':'station_id'})

# Join together
panel_60 = pd.merge(panel_60,trip60,how="left",left_on=['station_id','interval60'],right_on=['station_id','interval60']).fillna(0)

# Rename trip counts
panel_60['trip_id'] = panel_60['trip_id'].astype(int)
panel_60 = panel_60.rename(columns={'trip_id':'trip_count'})

# Find out the week: for further train/test set split
panel_60['week'] = panel_60['interval60'].dt.strftime("%W")
panel_60['tag'] = panel_60['week'].apply(lambda x: 'test' if ((x=='23')|(x=='24')) else 'train')

panel_60.head()

"""- #### 2.3.2 ***Final Panel***"""

# "Impute" missing values with the median value
missing = bike_data['percent_car'].isnull()
bike_data.loc[missing, 'percent_car'] = bike_data['percent_car'].median()

final_panel = pd.merge(panel_60,
                       bike_data[['start_station','totalDocks','percent_car',
                                  'totalpop','student_ratio','educated_ratio',
                                 'totalDocks','employed_ratio','logDistRests','within_centerCity',
                                 'logIntersectionDists','logStationDists',
                                 'laggedTrips']],
                      how='left',
                      left_on=['station_id'],
                      right_on=['start_station'])

# Drop duplicates
final_panel = final_panel.drop(columns=['start_station'])

final_panel.head()

# Write CSV
# final_panel.to_csv('final_panel.csv',index=0)

#all_bike_data = bike_data.drop(columns=['geometry'])
#all_bike_data.to_csv('bike_data.csv',index=0)

"""## **III. Modeling**

### 3.1 General Demand Prediction

The first part of our modeling is the general demand prediction, in which temporal features like time lags are not included, as we will use them in the latter models.

Aim for this part is to figure out what is(are) the most important features that affect bike share demand. First, we will use `Random Forest` and several `OLS` models to perform the prediction, then find out the best model, and draw the importance plot based on the selected model.

- Preprocess
"""

bike_data = pd.read_csv(r'C:\Users\yyp\Desktop\Upenn\MUSA\CPLN6800\bike_data.csv')

bike_data.head()

"""- Perform our test/train split"""

# Remove unnecessary columns
bike_features = bike_data.drop(labels=["name",'kioskId'], axis=1)

# Split the data 
train_set, test_set = train_test_split(
    bike_features,
    test_size=0.4,
    random_state=12345
)

# the target labels: log of total start trips
y_train = np.log(train_set["total_start_trips"])
y_test = np.log(test_set["total_start_trips"])

# Select Features
train_set = train_set.iloc[:,4:]
test_set = test_set.iloc[:,4:]

"""#### 3.1.1 **Random Forest**"""

# Setup the pipeline with a standard scaler
pipe = make_pipeline(
    StandardScaler(), RandomForestRegressor(random_state=42)
)
model_name = "randomforestregressor"
param_grid = {
    f"{model_name}__n_estimators": [5, 10, 25, 50, 100, 200, 300, 500],
    f"{model_name}__max_depth": [None, 2, 5, 7, 9, 13],
}

param_grid
# Create the grid and use 10-fold CV
grid = GridSearchCV(pipe, param_grid, cv=10)

# Run the search
grid.fit(train_set, y_train);
grid.best_params_
# Evaluate the best random forest model
best_random = grid.best_estimator_
grid.score(test_set, y_test)

"""#### 3.1.2 **OLS**

- ##### ***Linear Regression***
"""

from sklearn.linear_model import LinearRegression

# create a Linear Regression model object
lr_model = LinearRegression()

# fit the model to the training data
lr_model.fit(train_set, y_train)

# make predictions on the test data
y_pred_lr = lr_model.predict(train_set)

# evaluate the model performance
lr_score = lr_model.score(test_set, y_test)
lr_score

"""- ##### ***Ridge Regression***"""

from sklearn.linear_model import Ridge

# create a Ridge Regression model object
ridge_model = Ridge(alpha=1.0)

# fit the model to the training data
ridge_model.fit(train_set, y_train)

# make predictions on the test data
y_pred_ridge = ridge_model.predict(train_set)

# evaluate the model performance
ridge_score = ridge_model.score(test_set, y_test)
ridge_score

"""- ##### ***Lasso Regression***"""

from sklearn.linear_model import Lasso

# create a Lasso Regression model object
lasso_model = Lasso(alpha=1.0)

# fit the model to the training data
lasso_model.fit(train_set, y_train)

# make predictions on the test data
y_pred_lasso = lasso_model.predict(train_set)

# evaluate the model performance
lasso_score = lasso_model.score(test_set, y_test)
lasso_score

"""- ##### ***Elastic Net Regression***"""

from sklearn.linear_model import ElasticNet

# create an Elastic Net Regression model object
en_model = ElasticNet(alpha=1.0, l1_ratio=0.5)

# fit the model to the training data
en_model.fit(train_set, y_train)

# make predictions on the test data
y_pred_en = en_model.predict(train_set)

# evaluate the model performance
en_score = en_model.score(test_set, y_test)
en_score

"""#### 3.1.3 Importance Plot

Find out the most important features in the random forest model:
"""

# The best model
regressor = grid.best_estimator_["randomforestregressor"]

# Create the dataframe with importances
importance = pd.DataFrame(
    {"Feature": train_set.columns, "Importance": regressor.feature_importances_}
)

# Sort importance in descending order and get the top
importance = importance.sort_values("Importance", ascending=False)

# Plot
importance_plot = importance.hvplot.barh(x="Feature", y="Importance", flip_yaxis=True, height=300, dynamic=False)

#renderer = hv.renderer('bokeh')
#renderer.save(importance_plot, 'importance_plot')

"""#### 3.1.4 Conclusion

Judging from all the five models, we can say that the random forest performs the best, and the most important feature in the model is the lagged distance from nearby stations which is the same as the plot we made in the last part.

### 3.2 Time Series Prediction

#### 3.2.1 **ARIMA**

**ARIMA (Autoregressive Integrated Moving Average)** is a popular statistical method for time series forecasting. It combines three main components: Autoregressive (AR), Moving Average (MA), and Integration (I).

ARIMA models are represented as ARIMA(p, d, q), where p, d, and q are the orders of the AR, I, and MA components, respectively. Seasonal ARIMA models, known as SARIMA or SARIMAX (when including external variables), can also incorporate seasonal components to capture patterns that repeat at regular intervals, such as daily or yearly seasonality.

To make accurate predictions with ARIMA models, the appropriate model order (p, d, q) and, if applicable, seasonal order need to be determined. This is typically done using techniques like grid search, stepwise search, or auto-ARIMA methods, which identify the model with the best performance based on a chosen criterion, such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC).

Once the best ARIMA model is selected, it can be used to forecast future values in the time series. The model's performance can be evaluated using metrics like Mean Absolute Error (MAE), which provide insights into the accuracy of the predictions.
"""

# Read File
bike_df = pd.read_csv(r'C:\Users\yyp\Desktop\Upenn\MUSA\CPLN6800\final_panel.csv')
bike_df = bike_df.drop('totalDocks.1', axis=1)

# Select Station3:`3022`
station_3022_df = bike_df[bike_df['station_id'] == 3022]

station_3022_df

station_3022_df['interval60'] = pd.to_datetime(station_3022_df['interval60'])

fig, ax = plt.subplots(figsize=(15, 3))


unique_days = station_3022_df['interval60'].dt.normalize().unique()
colors = cm.rainbow(np.linspace(0, 1, len(unique_days)))


for day, color in zip(unique_days, colors):
    day_data = station_3022_df[station_3022_df['interval60'].dt.normalize() == day]
    ax.plot(day_data['interval60'], day_data['trip_count'], color=color, label=pd.Timestamp(day).strftime('%Y-%m-%d'))

ax.set_xlabel('Date')
ax.set_ylabel('Trip Count')
ax.set_title('Trip Count Time Series for Station 3022 (Amtrak 30th Street Station) by Day')


date_range = pd.date_range(start=station_3022_df['interval60'].min(), end=station_3022_df['interval60'].max(), freq='D')
ax.set_xticks(date_range)
ax.set_xticklabels(date_range.strftime('%Y-%m-%d'), rotation=45)
legend = ax.legend(ncol=len(unique_days)//5, bbox_to_anchor=(0.5, -0.45), loc='lower center', borderaxespad=0.)
for line in legend.get_lines():
    line.set_linewidth(2)
plt.show()

"""- ##### ***Step1***: To find the order of differencing(d) in ARIMA model

The order of differencing in ARIMA model depends on whether the time series is stationary or not. The Augmented Dickey Fuller test (`adfuller()`) can be used to check the stationarity of the series. If the p-value of the test is less than the significance level (0.05), then we reject the null hypothesis and conclude that the series is stationary. In this case, no differencing is needed (d=0). However, if the p-value is greater than 0.05, then the series is non-stationary and differencing is required.
"""

trip_ARIMA_df = station_3022_df[['trip_count','interval60']].set_index('interval60')
trip_ARIMA_df

# %% ADF Test 
test = trip_ARIMA_df['trip_count'].dropna()
result = adfuller(test.values, autolag='AIC') 
print(f'ADF Statistic: {result[0]}') 
print(f'p-value: {result[1]}') 
for key, value in result[4].items(): 
    print('Critial Values:') 
    print(f'   {key}, {value}')

"""p-value is smaller than the significance level, we infer that the time series is stationary. Therefore, the order of differencing d is set to 0.

- ##### ***Step2***:To find the order of AR term (p) and MA term (q)

There are basically two ways to determine the optimal value of AR term (p):

1. Manual selection: You can manually select the AR term (p) by plotting the autocorrelation function (ACF) and partial autocorrelation function (PACF). This requires expertise and experience.

2. Automatic selection: You can use the auto_arima function in Python to automatically select the best values for the parameters p and q. The auto_arima function compares the likelihood of the model's effectiveness (which can be related to accuracy in later prediction) to select the best values for p and q.

Therefore, using the auto_arima function can save time in manually selecting parameters and ensure that the selected parameters are optimal.
"""

# Seperate the data into train and test data 
station_3022_df['interval60'] = pd.to_datetime(station_3022_df['interval60'])
station_3022_df.set_index('interval60', inplace=True)
bike_train = station_3022_df.loc[station_3022_df['tag'] == 'train']
bike_test = station_3022_df.loc[station_3022_df['tag'] == 'test']

"""Next we will build a model for ARIMA. Here's the explanation of the parameters used in the auto_arima() function:

1. start_p: Initial value of p (AR parameter) to start with.
2. start_q: Initial value of q (MA parameter) to start with.
3. test: Type of test to use for finding the optimal value of d (differencing parameter) using the augmented Dickey-Fuller test. Here, we're using the ADF test.
4. max_p: Maximum value of p to consider during model selection.
5. max_q: Maximum value of q to consider during model selection.
6. m: Frequency of the series. Default is 1 for non-seasonal data.
7. d: The order of differencing to be done on the series. If this is None, then the function tries to find the optimal value of d using the specified test.
8. seasonal: Boolean indicating whether the data has seasonality or not. Here, we're assuming that the data is non-seasonal.
"""

# Seasonal - fit stepwise auto-ARIMA 
smodel = pm.auto_arima(bike_train['trip_count'],
                             start_p=1, start_q=1,
                             test='adf',
                             max_p=3, max_q=3,
                             m=12,  # Here m denotes the seasonal period, for example, if the data is hourly, then there are 24 hours in a day, so m = 24
                             d=None, D=None,
                             seasonal=True,  # Set seasonal=True to enable seasonality
                             start_P=0, start_Q=0,
                             max_P=3, max_Q=3,
                             trace=True,
                             error_action='ignore',
                             suppress_warnings=True,
                             stepwise=True)

print(smodel.summary())

"""From the results above, we can see that the best-fit model is ARIMA(3, 0, 0). This means that the order of the autoregressive (AR) term is 1, the order of the difference (d) is 0, and the order of the moving average (MA) term is 3.

The AIC (Abject Pool Information Criterion) value of the model is 2009.388, which is a measure of the goodness of fit of the model, and a lower AIC value usually implies a better fit.

- ##### ***Step 3***: Automatically build SARIMA model in python

Seasonal differencing is similar to regular differencing, but, instead of subtracting consecutive terms, you subtract the value from the previous season.

So, the model will be represented as SARIMA(p,d,q)x(P,D,Q), where, P, D and Q are SAR, order of seasonal differencing and SMA terms respectively and 'x' is the frequency of the time series.

If your model has well defined seasonal patterns, then enforce D=1 for a given frequency ‘x’.

Here’s some practical advice on building SARIMA model:

As a general rule, set the model parameters such that D never exceeds one. And the total differencing ‘d + D’ never exceeds 2. Try to keep only either SAR or SMA terms if your model has seasonal components.

Therefore, we use the same way to automatically identify the parameters of the SARIMA model.
"""

# Seasonal - fit stepwise auto-ARIMA 
smodel = pm.auto_arima(bike_train['trip_count'],
                             start_p=1, start_q=1,
                             test='adf',
                             max_p=4, max_q=4,
                             m=8,  # Here m denotes the seasonal period, for example, if the data is hourly, then there are 24 hours in a day, so m = 24
                             d=None, D=1,
                             seasonal=True,  # Set seasonal=True to enable seasonality
                             start_P=0, start_Q=0,
                             max_P=4, max_Q=4,
                             trace=True,
                             error_action='ignore',
                             suppress_warnings=True,
                             stepwise=True)

print(smodel.summary())

"""We obtained the best-fitting model as a seasonal ARIMA model: ARIMA(3, 0, 0)(2, 0, 0)[24]. This means that:

1. The order of the autoregressive (AR) terms is 3: ar.L1, ar.L2, and ar.L3.

2. The order of differencing (d) is 0: the model does not apply differencing.

3. The order of the moving average (MA) terms is 0: the model does not use moving average terms.
Additionally, the model has a seasonal component:

4. The order of the seasonal autoregressive (SAR) terms is 2: ar.S.L24 and ar.S.L48.

5. The seasonal order of differencing is 0.

6. The seasonal order of the moving average (SMA) terms is 0.
The model uses a period of 24 (the last parameter in the order), meaning that it captures the daily seasonal variations in the data.

By incorporating the seasonal component, this model might predict the demand for bike stations more accurately than the previous non-seasonal ARIMA(1, 0, 3) model. We can use this seasonal ARIMA model to forecast the data in the test set and then evaluate the accuracy of the predictions.

- ##### ***Step 4***: Data forecasting using the model we created

Now let's forecast based on train data and compare the results to test data
"""

n_periods = len(bike_test)
forecast, conf_int = smodel.predict(n_periods=n_periods, return_conf_int=True)
index_of_fc = bike_test.index

# make series for plotting purpose
forecast_series = pd.Series(forecast, index=index_of_fc)
lower_series = pd.Series(conf_int[:, 0], index=index_of_fc)
upper_series = pd.Series(conf_int[:, 1], index=index_of_fc)

# Plot
fig = plt.figure(figsize = (20,4))
plt.plot(bike_train['trip_count'], label='Train')
plt.plot(bike_test['trip_count'], color='darkred', label='Test')
plt.plot(forecast_series, color='darkgreen', label= 'Prediction')

plt.fill_between(lower_series.index, 
                 lower_series, 
                 upper_series, 
                 color='k', alpha=.15)

plt.title("SARIMA - Forecast of Bike Station Demand")
plt.legend()
plt.show()

#Evaluation
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
mae = mean_absolute_error(forecast_series, bike_test['trip_count']) 
r2 = r2_score(forecast_series, bike_test['trip_count'])
mse = mean_squared_error(forecast_series, bike_test['trip_count'])
print("Mean Absolute Error:",mae)
print("Mean Squared Error:",mse)
print("r2_score:",r2)

"""#### 3.2.2 **GRU**

Recurrent neural networks (RNNs) are ofren leveraged to model the temporal dependency. In particular, we use Gated Recurrent Units (GRU), which is a powerful variant of RNNs overcoming gradient vanishing and gradient exploding problem effectively. Encoder-decoder structure is widely used in spatiotemporal sequence predicting tasks because it has been verified very effective. Therefore, we introduce encoder-decoder structure to build sequence to sequence architecture along with GRU units. Sequence to sequence is effective in multiple steps ahead prediction.

**Gated Recurrent Unit**. Recurrent Neural Networks (RNNs) can model the dependency of time series effectively. However, the traditional RNN models have limitations for long-term prediction due to gradient vanishing and gradient exploding problems. Although Long Short-term Memory (LSTM) can address these challenges, it has the defect of more training time consumption special for complex structures. Hence, we introduce Gated Recurrent Unit (GRU), the variant of RNNs, to model the temporal dependency. Compared with LSTM, GRU model has a relatively simple structure with fewer parameters and faster training speed.
"""

# Set up `GPU`:
device = ('cuda'if torch.cuda.is_available else 'cpu')

# Adjacancy Matrix
adj_path = r'./distance_matrix.csv'

adj0 = pd.read_csv(adj_path)
valid = adj0.columns

# Time Panel
panel_60 = pd.read_csv(r'./panel_60.csv')
panel = panel_60[valid]

"""- ##### Train/Test Set Split"""

data = panel

seq_len = 504 # 3 weeks'
pre_len = 1 # 1 hour's
split_ratio = 0.8
time_len = panel.shape[0]

max_val = np.max(data) # Uniform
data = data / max_val

train_size = int(time_len * split_ratio)
train_data = data[:train_size]

train_X, train_Y, test_X, test_Y = [], [], [], []

for i in range(len(train_data) - seq_len - pre_len+1):
        train_X.append((data[i : i + seq_len]))
        train_Y.append((data[i + seq_len : i + seq_len + pre_len]))

for i in range(len(train_data) - seq_len - pre_len, len(data) - seq_len - pre_len):
        test_X.append((data[i : i + seq_len]))
        test_Y.append((data[i + seq_len : i + seq_len + pre_len]))

# Hyperparameters
INPUT_SIZE = 139
HIDDEN_SIZE = 64
LEARN_RATE = 1e-3
EPOCH = 100
# BATCH_SIZE = 32

# To Tensor
train_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(np.array(train_X)), torch.FloatTensor(np.array(train_Y))
    )

test_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(np.array(test_X)), torch.FloatTensor(np.array(test_Y))
    )

"""- ##### ***Module: GRU***"""

class GRU(nn.Module):
    def __init__(self):
        super(GRU, self).__init__()
        self.gru = nn.GRU(INPUT_SIZE, HIDDEN_SIZE, batch_first=True)
        self.mlp = nn.Sequential(
            nn.Linear(HIDDEN_SIZE, 32),
            nn.LeakyReLU(),
            nn.Linear(32, 16),
            nn.LeakyReLU(),
            nn.Linear(16, 139)
        )

    def forward(self, x):
        out, _ = self.gru(x)
        out = out[:,:][-1]
        out = self.mlp(out)
        return out

"""- ##### Train"""

import time

net_gru = GRU().to(device)

optim_gru = optim.Adam(params=net_gru.parameters(), lr=LEARN_RATE)

loss_fuc = nn.MSELoss()


def train_for_gru(data, loss_fuc, net, optim, Epoch):
      for epoch in range(EPOCH):
        loss_print = []
        for i, (x, y) in enumerate(data):
            x, y = x.to(device), y.to(device)
            optim_gru.zero_grad()
            y_pred = net(x)
            loss = loss_fuc(y_pred, y)
            loss_print.append(loss.item())
            loss.backward()
            optim_gru.step()
        print('GRU:loss:', sum(loss_print)/len(train_dataset))

def main():
    start = time.perf_counter()
    train_for_gru(train_dataset, loss_fuc, net_gru, optim_gru, EPOCH)
    end = time.perf_counter()
    print('Training Time: {:.2f}s'.format(end-start))
    #Save
    torch.save(net_gru.state_dict(), 'gru.pt')

main()

"""- ##### Test"""

def y_pred_to_numpy(t):
    t = t.detach().cpu().numpy()
    return t

net_gru = GRU().to(device)
net_gru.load_state_dict(torch.load('gru.pt'))

def test_for_gru(data):
    diff = pd.DataFrame()
    true_y = pd.DataFrame()
    pred_y = pd.DataFrame()
    #mae = []
    #mse = []
    #rmse = []
    #r_squared = []
    #with torch.no_grad():
    for i, (x, y) in enumerate(data):
        y_pred = net_gru(x.to(device))
        y_pred = y_pred_to_numpy(y_pred)
        y_pred = y_pred*max_val
        test_y = y
        test_y = y.to(device)
        test_y = y_pred_to_numpy(y)
        test_y = test_y.reshape(139,)
        test_y = test_y*max_val
        dif = pd.DataFrame(np.abs((test_y-y_pred)))
        diff = pd.concat([diff,dif],axis=1)
        #mse.append(mean_squared_error(y_pred, test_y))
        #mae.append(mean_absolute_error(y_pred, test_y))
        #rmse.append(np.sqrt(mean_squared_error(y_pred, test_y)))
        #r_squared.append(r2_score(test_y, y_pred))
        y_true = pd.DataFrame(test_y)
        true_y = pd.concat([true_y,y_true],axis=1)
        y_pred = pd.DataFrame(y_pred)
        pred_y = pd.concat([pred_y,y_pred],axis=1)
    return(diff, true_y, pred_y)

MAE2, true, pred = test_for_gru(test_dataset)

# True value for station 3022
true_3022 = true.T[['3022']]

# True value for station 3022
pred_3022 = pred.T[['3022']]

gru_3022_mse = mean_squared_error(pred_3022, true_3022)
gru_3022_mae = mean_absolute_error(pred_3022, true_3022)

print(f"GRU: MSE = {mean_squared_error(pred_3022, true_3022)}")
print(f"GRU: MAE = {mean_absolute_error(pred_3022, true_3022)}")

"""- ##### ***Plot***: *Amtrak 30th Street Station* Prediction: GRU"""

fig, ax = plt.subplots(figsize=(15, 3))
x = np.linspace(0, 168, 168)
plt.plot(x, true_3022, '-o', color='#bdc9e1', alpha = 0.8, markerfacecolor='#4d004b',markersize = 1.5)
plt.plot(x, pred_3022,'-o', color='#6a51a3', alpha = 0.8, markerfacecolor='white', markersize = 1.2)
# plt.xticks([])
plt.legend(('Ground Truth', 'Prediction:{}'.format("GRU")), loc='upper right')
ax.set_title('Amtrak 30th Street Station',
                 fontsize='small',
                 loc='right',
                 #style='italic',
                 family='monospace')

fig.suptitle(
        "Prediction of Indego Shared Bikes Ridership : GRU",
        fontsize='large',
        #loc='center',
        style='italic',
        fontweight='bold',
        family='monospace')

ax.set_yticks([0,2,4,6,8,10])
ax.set_xticklabels(['0','6-14', '6-15', '6-16', '6-17', '6-18', '6-19', '6-20','6-21'])
ax.spines[:].set_visible(False)
ax.grid(True, 'major', 'y', lw=.5)
ax.grid(True, 'major', 'x', ls='--',lw=.5)
plt.show()

row_mean = MAE2.mean(axis=1)

MAE3 = pd.DataFrame(row_mean)
MAE3 = MAE3.reset_index()
MAE3.columns = ['station_id','GRU_MAE']

MAE3.loc[MAE3['station_id']=='3022']

"""#### 3.2.3 GCN

In spatial correlation modeling, current works usually employ grid to divide the urban area, converting urban data to Euclidean domains, and then use Convolutional Neural Networks (CNN) to model spatial correlation. However, in our problem, the spatial distribution of bike stations is irregular and in non-Euclidean domains. Hence, we introduce graph structure to model the spatial distribution of bike stations and exploit Graph Convolutional Networks (GCN) to model spatial correlation.

**Graph Convolutional Network(GCN)**, is a deep learning model used to handle graph-structured data. Compared to traditional neural networks, GCN can directly process graph-structured data such as social networks, knowledge graphs, chemical molecules, etc.

The basic idea of GCN is to combine the representations of nodes and their neighboring nodes and update the node representations through convolutional operations, thereby achieving feature learning and representation of the entire graph. GCN is a method based on spectral graph theory, which decomposes the ***adjacency matrix*** of the graph into eigenvalues to obtain the Laplacian matrix of the graph, and thus converts the convolutional operation into matrix multiplication. This method can effectively capture topological structure information between nodes and achieve graph feature learning and representation.

- ##### Adjacancy Matrix

- load
"""

# Adjacancy Matrix
adj_path = r'./distance_matrix.csv'

adj0 = pd.read_csv(adj_path)
valid = adj0.columns

def load_adjacency_matrix(adj_path, dtype=np.float32):
    adj_df = pd.read_csv(adj_path, header=None)
    adj_df = adj_df.iloc[1: , :]
    adj = np.array(adj_df, dtype=dtype)
    return adj

adj = load_adjacency_matrix(adj_path, dtype=np.float32)

"""- Normalize"""

# Cast the `station_id` to `int`
valid = pd.DataFrame(valid)
valid.columns = ['station_id']
valid['station_id'] = valid['station_id'].astype(int)

# to tensor
tensor_adj = torch.from_numpy(adj)

# Function to calculate laplacian matrix
def calculate_laplacian_with_self_loop(matrix):
    matrix = matrix + torch.eye(matrix.size(0))
    row_sum = matrix.sum(1)
    d_inv_sqrt = torch.pow(row_sum, -0.5).flatten()
    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.0
    d_mat_inv_sqrt = torch.diag(d_inv_sqrt)
    normalized_laplacian = (
        matrix.matmul(d_mat_inv_sqrt).transpose(0, 1).matmul(d_mat_inv_sqrt)
    )
    return normalized_laplacian

nomalized_adj = calculate_laplacian_with_self_loop(tensor_adj)

"""- ##### ***Module: GCN***"""

class GCN(nn.Module):
    def __init__(self, adj, input_dim: int, output_dim: int, **kwargs):
        super(GCN, self).__init__()
        self.register_buffer(
            "laplacian", calculate_laplacian_with_self_loop(torch.FloatTensor(adj))
        )
        self._num_nodes = adj.shape[0]
        self._input_dim = 168  # seq_len for prediction
        self._output_dim = 168  # hidden_dim for prediction
        self.weights = nn.Parameter(
            torch.FloatTensor(self._input_dim, self._output_dim)
        )
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.weights, gain=nn.init.calculate_gain("tanh"))

    def forward(self, inputs):
        inputs = inputs.T # (168,139)--->(139,168)、
        ax = self.laplacian @ inputs # (139,139)*(139,168)--->(139,168)
        ax = ax.reshape((self._num_nodes, self._input_dim))#(139,168)
        # act(AXW) 
        outputs = torch.tanh(ax @ self.weights) # (139,168)*(168,168)--->(139,168)
        outputs = outputs.reshape((self._num_nodes, self._output_dim)) # (139,168)
        # (num_nodes, output_dim)
        outputs = outputs.transpose(0, 1)
        return outputs

"""- ##### Transformation"""

# Select the test sets: the last week(6/14-6.20)
test_slice = data[-168:]

# To tensor
test_slice = test_slice.reset_index(drop=True)
test = torch.tensor(test_slice.values)

# Set the gcn model:
net_gcn = GCN(adj, 168, 168).to(device)

# Define the function to perform GCN
def calculation_gcn(dataset, net):
  data = torch.tensor(dataset.to_numpy(),dtype=torch.float)
  y_pred = net(dataset.to(device))   
  return(y_pred)

# Get the results:
slice_pred = calculation_gcn(test_slice,net_gcn)

# transform to a dataframe & rename the columns:
sliced_gcn = pd.DataFrame(y_pred_to_numpy(slice_pred))
sliced_gcn.columns = adj0.columns

# True value for station 3022:
slice_3022_true = test_slice[['3022']]

# Prediction for station 3022:
slice_3022_pred = sliced_gcn[['3022']]

# MAE:
gcn_3022_mae = mean_absolute_error(slice_3022_pred, slice_3022_true)

# MSE:
gcn_3022_mse = mean_squared_error(slice_3022_pred, slice_3022_true)

print(f"GCN: MSE = {gcn_3022_mse}")
print(f"GCN: MAE = {gcn_3022_mae}")

"""- ##### ***Plot***: Amtrak 30th Street Station Prediction: GCN"""

fig, ax = plt.subplots(figsize=(15, 3))
x = np.linspace(0, 168, 168)
plt.plot(x, slice_3022_true, '-o', color='#bdc9e1', alpha = 0.8, markerfacecolor='#4d004b',markersize = 1.5)
plt.plot(x, slice_3022_pred,'-o', color='#6a51a3', alpha = 0.8, markerfacecolor='white', markersize = 1.2)
# plt.xticks([])
plt.legend(('Ground Truth', 'Prediction:{}'.format("GCN")), loc='upper right')
ax.set_title('Amtrak 30th Street Station',
                 fontsize='small',
                 loc='right',
                 #style='italic',
                 family='monospace')

fig.suptitle(
        "Prediction of Indego Shared Bikes Ridership : GCN",
        fontsize='large',
        #loc='center',
        style='italic',
        fontweight='bold',
        family='monospace')

ax.set_yticks([0,2,4,6,8,10])
ax.set_xticklabels(['0','6-14', '6-15', '6-16', '6-17', '6-18', '6-19', '6-20','6-21'])
ax.spines[:].set_visible(False)
ax.grid(True, 'major', 'y', lw=.5)
ax.grid(True, 'major', 'x', ls='--',lw=.5)
plt.show()

"""#### 3.2.4 Comparison

In this section, we compare the 2 neural network models. The metric we take here is ***MAE***. We have calculated it for the following two methods separatel:

1. ***MAE*** of all stations by time interval(1h):

  - To explore the temporal pattern of MAE;

2. ***MAE*** by each station.

  - To explore the spatial pattern of MAE.

---------------------------

From the figures, we can see that for ***GRU***, **MAE** changes periodically, generally having the lowest point in the morning, while in the afternoon, the number becomes bigger, which may be resulted from the fact that people ride more bikes in the afternoon.

For ***GCN***, the magnitude of the value change is not very large, basically around 0.8-1.0. However, this fact also indicates the weakness of the ***GCN*** model, even though it is stable.

Both advantages and shortcomings of the two models are obvious. Perhaps the next step is to combine the two to see if better results can be achieved.

- **As colab cannot display `hvplot`, so the picture could not be seen here, please check the website. Thank you!**

- ##### ***MAE***: Interval

- time interval
"""

# Get time interval (test set: 168 intervals)
time1 = panel_60[['interval60']][-168:].reset_index(drop=True).reset_index()

time1.head(n=3)

"""- GCN: MAE for interval"""

# Calculate Absolute Error for each station every interval
AE_GCN = pd.DataFrame(np.abs(sliced_gcn-test_slice))

# Calculate MAE for all stations each time interval
gcn_interval_mae = pd.DataFrame(AE_GCN.mean(axis=1))

# Reset index
gcn_interval_mae = gcn_interval_mae.reset_index()

# Rename columns
gcn_interval_mae.columns=['interval','GCN_interval_MAE']

# Merge: to get the exact time interval
gcn_interval_mae = pd.merge(time1,
               gcn_interval_mae, 
               how='left',left_on=['index'],
               right_on=['interval'],).drop(columns=['interval','index'])

gcn_interval_mae

"""- GRU: MAE for interval"""

row_mean = MAE2.mean(axis=0)

gru_interval_mae = pd.DataFrame(row_mean)
gru_interval_mae = gru_interval_mae.reset_index(drop=True).reset_index()
gru_interval_mae.columns = ['interval','GRU_interval_MAE']

gru_interval_mae = pd.merge(time1,
               gru_interval_mae, 
               how='left',left_on=['index'],
               right_on=['interval'],).drop(columns=['interval','index'])

"""- Merge"""

Interval_MAE = pd.merge(gru_interval_mae,
             gcn_interval_mae,
             how='left',
             on='interval60')

Interval_MAE.columns=['interval60','GRU','GCN']

Interval_MAE['interval60'] = Interval_MAE['interval60'].astype("datetime64[ns]")

Interval_MAE_hvplot = Interval_MAE.hvplot.line(x='interval60', 
                y=['GRU','GCN'], 
                value_label='MAE', 
                legend='top', 
                color=['#bdc9e1', '#6a51a3'],
                height=300, width=360)

#Interval_MAE.to_csv('Interval_MAE.csv',index=0)
#hvplot.save(Interval_MAE_hvplot, 'Interval_MAE_hvplot.html')

"""- ##### ***MAE***: stations

The scatter patterns of the two models also suggest what we have discovered in the above interval MAE check. 

One thing the two have in common is that they tended to have larger ***MAE*** in center city, where the stations generally have more docks and more rides which is correspondent with the above. For the accuracy, ***GRU*** performs better on most stations than ***GCN***.

- station
"""

# API endpoint
API_endpoint = "http://www.rideindego.com/stations/json"

# Get the JSON
stations_geojson = requests.get(API_endpoint).json()

# Convert to a GeoDataFrame
stations = gpd.GeoDataFrame.from_features(stations_geojson, crs="EPSG:4326")

"""- City Limit"""

# Download the Philadelphia city limits
url = "https://services.arcgis.com/fLeGjb7u4uXqeF9q/arcgis/rest/services/City_Limits/FeatureServer/0"
city_limits = esri2gpd.get(url).to_crs(epsg=3857)

# Get the geometry from the city limits
city_limits_outline = city_limits.to_crs(epsg=4326).squeeze().geometry

"""- GCN"""

# Calculate MAE for all stations each time interval
gcn_station_mae = pd.DataFrame(AE_GCN.mean(axis=0))

# Reset index
gcn_station_mae = gcn_station_mae.reset_index()

# Rename columns
gcn_station_mae.columns=['station_id','GCN_station_MAE']

# Change Type
gcn_station_mae['station_id'] = gcn_station_mae['station_id'].astype(int)

#gcn_station_mae.to_csv('./0425_gcn_station_mae.csv',index=0)

gcn_station_mae_gdf = pd.merge(gcn_station_mae,stations[['geometry','id','name','totalDocks','latitude','longitude']],how='left',left_on=['station_id'],right_on=['id'])
gcn_station_mae_gdf = gpd.GeoDataFrame(gcn_station_mae_gdf,crs="EPSG:4326", geometry='geometry')

MAE6 = gcn_station_mae_gdf.to_crs(3857)

background = alt.Chart(city_limits).mark_geoshape(
    fill='lightgray',
    stroke='white'
).project(
    type='identity', 
      reflectY=True).properties(
    width=360,
    height=360)
      
gcn_station_alt = alt.Chart(MAE6).\
      mark_geoshape().\
      encode(
      tooltip=['name:N','station_id:Q', 'GCN_station_MAE:Q'],
      color=alt.Color('GCN_station_MAE:Q', scale=alt.Scale(scheme="greenblue"),title='MAE')).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='MAE for Each Bike Station, GCN')

gcn_mae = background + gcn_station_alt

gcn_mae

"""- GRU"""

row_mean = MAE2.mean(axis=1)

gru_station_mae = pd.DataFrame(row_mean)
gru_station_mae = gru_station_mae.reset_index()
gru_station_mae.columns = ['station_id','GRU_station_MAE']

#gru_station_mae.to_csv('./0425_gru_station_mae.csv',index=0)

MAE3 = gru_station_mae
MAE3['station_id'] = MAE3['station_id'].astype(int)

MAE4 = pd.merge(MAE3,stations[['geometry','id','name','totalDocks','latitude','longitude']],how='left',left_on=['station_id'],right_on=['id'])
MAE4 = gpd.GeoDataFrame(MAE4, crs="EPSG:4326", geometry='geometry')

MAE5 = MAE4.to_crs(3857)

background = alt.Chart(city_limits).mark_geoshape(
    fill='lightgray',
    stroke='white'
).project(
    type='identity', 
      reflectY=True).properties(
    width=360,
    height=360)
      
station_alt = alt.Chart(MAE5).\
      mark_geoshape().\
      encode(
      tooltip=['name:N','station_id:Q', 'GRU_station_MAE:Q'],
      color=alt.Color('GRU_station_MAE:Q', scale=alt.Scale(scheme="greenblue"),title='MAE')).\
      project(
      type='identity', 
      reflectY=True).\
      properties(
      width=360, 
      height=360,
      title='MAE for Each Bike Station, GRU')

gru_mae = background + station_alt

gru_mae

"""- Tabs"""

choro_1 = pn.panel(gru_mae)
choro_2 = pn.panel(gcn_mae)

tabs = pn.Tabs(
    ('GRU', choro_1),
    ('GCN', choro_2)
)
tabs

#tabs.save('0425_MAE.html')

"""## **IV. Analysis**

In this study, we select 35 days' data of Indego Bike trips in 2021 summer to predict the demand of the shared bike demand in Philadelphia.

Based on models we have made, we found out for general demand, the distance to nearby shared bike stations and the number of trips in nearby stations serve as a significant role in the demand. Besides, for time series prediction, we use ***MAE*** as the final metric to evaluate our 3 models: ***ARIMA***, ***GRU***, ***GCN***. 

As the ***GRU*** model has the lowest **MAE** (0.98), it outperforms the other two. We would recommend this model to the Department of Transportation for further planning, and also as a support to solve the re-balancing problem.
"""